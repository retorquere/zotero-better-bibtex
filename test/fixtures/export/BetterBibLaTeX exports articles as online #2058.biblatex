@article{vandenoordDummyWaveNetGenerative2016,
  title = {Dummy - {{WaveNet}}: {{A Generative Model}} for {{Raw Audio}}},
  shorttitle = {{{WaveNet}}},
  author = {family=Oord, given=Aaron, prefix=van den, useprefix=true and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  date = {2016-09-19},
  journaltitle = {dummy},
  eprint = {1609.03499},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/1609.03499},
  abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  langid = {english},
  keywords = {Article,Computer Science - Machine Learning,Computer Science - Sound}
}

@unpublished{vandenoordWaveNetGenerativeModel2016,
  title = {{{WaveNet}}: {{A Generative Model}} for {{Raw Audio}}},
  shorttitle = {{{WaveNet}}},
  author = {family=Oord, given=Aaron, prefix=van den, useprefix=true and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  date = {2016-09-19},
  eprint = {1609.03499},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1609.03499},
  abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  langid = {english},
  keywords = {Article,Computer Science - Machine Learning,Computer Science - Sound}
}

@unpublished{yangTorchAudioBuildingBlocks2022,
  ids = {yang2021torchaudio},
  title = {{{TorchAudio}}: {{Building Blocks}} for {{Audio}} and {{Speech Processing}}},
  shorttitle = {{{TorchAudio}}},
  author = {Yang, Yao-Yuan and Hira, Moto and Ni, Zhaoheng and Chourdia, Anjali and Astafurov, Artyom and Chen, Caroline and Yeh, Ching-Feng and Puhrsch, Christian and Pollack, David and Genzel, Dmitriy and Greenberg, Donny and Yang, Edward Z. and Lian, Jason and Mahadeokar, Jay and Hwang, Jeff and Chen, Ji and Goldsborough, Peter and Roy, Prabhat and Narenthiran, Sean and Watanabe, Shinji and Chintala, Soumith and Quenneville-BÃ©lair, Vincent and Shi, Yangyang},
  date = {2022-02-16},
  eprint = {2110.15018},
  eprinttype = {arXiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2110.15018},
  abstract = {This document describes version 0.10 of TorchAudio: building blocks for machine learning applications in the audio and speech processing domain. The objective of TorchAudio is to accelerate the development and deployment of machine learning applications for researchers and engineers by providing off-the-shelf building blocks. The building blocks are designed to be GPU-compatible, automatically differentiable, and production-ready. TorchAudio can be easily installed from Python Package Index repository and the source code is publicly available under a BSD-2-Clause License (as of September 2021) at https://github.com/pytorch/audio. In this document, we provide an overview of the design principles, functionalities, and benchmarks of TorchAudio. We also benchmark our implementation of several audio and speech operations and models. We verify through the benchmarks that our implementations of various operations and models are valid and perform similarly to other publicly available implementations.},
  keywords = {Article,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  note = {Comment: Accepted by ICASSP 2022}
}
