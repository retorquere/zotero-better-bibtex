{
  "config": {
    "id": "36a3b0b5-bad0-4a04-b79b-441c7cef77db",
    "label": "BetterBibTeX JSON",
    "localeDateOrder": "mdy",
    "options": {
      "exportNotes": true
    },
    "preferences": {
      "biblatexExtendedNameFormat": true,
      "citekeyFormat": "LibraryCatalog.match(/arxiv/i).discard + '_preprint' + auth.lower.postfix(_)  + ShortTitle.clean.lower.condense(_).postfix(_).len + year\n| auth.lower.postfix(_) + shorttitle(2,0).lower.condense(_).postfix(_) + year",
      "keyConflictPolicy": "change",
      "verbatimFields": "url,doi,file,ids,eprint,verba,verbb,verbc,groups"
    }
  },
  "items": [
    {
      "abstractNote": "Many tasks in modern machine learning can be formulated as finding equilibria in \\emph{sequential} games. In particular, two-player zero-sum sequential games, also known as minimax optimization, have received growing interest. It is tempting to apply gradient descent to solve minimax optimization given its popularity and success in supervised learning. However, it has been noted that naive application of gradient descent fails to find some local minimax and can converge to non-local-minimax points. In this paper, we propose \\emph{Follow-the-Ridge} (FR), a novel algorithm that provably converges to and only converges to local minimax. We show theoretically that the algorithm addresses the notorious rotational behaviour of gradient dynamics, and is compatible with preconditioning and \\emph{positive} momentum. Empirically, FR solves toy minimax problems and improves the convergence of GAN training compared to the recent minimax optimization algorithms.",
      "accessDate": "2019-12-18T11:40:04Z",
      "creators": [
        {
          "creatorType": "author",
          "firstName": "Yuanhao",
          "lastName": "Wang"
        },
        {
          "creatorType": "author",
          "firstName": "Guodong",
          "lastName": "Zhang"
        },
        {
          "creatorType": "author",
          "firstName": "Jimmy",
          "lastName": "Ba"
        }
      ],
      "date": "2019-11-25",
      "extra": [
        "arXiv: 1910.07512"
      ],
      "itemID": 1,
      "itemType": "journalArticle",
      "libraryCatalog": "arXiv.org",
      "notes": [
        "Comment: 21 pages"
      ],
      "publicationTitle": "arXiv:1910.07512 [cs, math, stat]",
      "shortTitle": "On Solving Minimax Optimization Locally",
      "tags": [
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Mathematics - Optimization and Control",
          "type": 1
        },
        {
          "tag": "Statistics - Machine Learning",
          "type": 1
        }
      ],
      "title": "On Solving Minimax Optimization Locally: A Follow-the-Ridge Approach",
      "url": "http://arxiv.org/abs/1910.07512"
    },
    {
      "abstractNote": "Many tasks in modern machine learning can be formulated as finding equilibria in sequential games. In particular, two-player zero-sum sequential games, also known as minimax optimization, have...",
      "accessDate": "2022-02-08T17:09:35Z",
      "conferenceName": "International Conference on Learning Representations",
      "creators": [
        {
          "creatorType": "author",
          "firstName": "Yuanhao",
          "lastName": "Wang*"
        },
        {
          "creatorType": "author",
          "firstName": "Guodong",
          "lastName": "Zhang*"
        },
        {
          "creatorType": "author",
          "firstName": "Jimmy",
          "lastName": "Ba"
        }
      ],
      "date": "2019/09/25",
      "extra": [
        "00060"
      ],
      "itemID": 2,
      "itemType": "conferencePaper",
      "language": "en",
      "libraryCatalog": "openreview.net",
      "shortTitle": "On Solving Minimax Optimization Locally",
      "title": "On Solving Minimax Optimization Locally: A Follow-the-Ridge Approach",
      "url": "https://openreview.net/forum?id=Hkx7_1rKwS"
    },
    {
      "abstractNote": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.",
      "accessDate": "2021-09-25T17:53:08Z",
      "creators": [
        {
          "creatorType": "author",
          "firstName": "Takeru",
          "lastName": "Miyato"
        },
        {
          "creatorType": "author",
          "firstName": "Toshiki",
          "lastName": "Kataoka"
        },
        {
          "creatorType": "author",
          "firstName": "Masanori",
          "lastName": "Koyama"
        },
        {
          "creatorType": "author",
          "firstName": "Yuichi",
          "lastName": "Yoshida"
        }
      ],
      "date": "2018-02-16",
      "extra": [
        "arXiv: 1802.05957"
      ],
      "itemID": 3,
      "itemType": "journalArticle",
      "libraryCatalog": "arXiv.org",
      "notes": [
        "Comment: Published as a conference paper at ICLR 2018"
      ],
      "publicationTitle": "arXiv:1802.05957 [cs, stat]",
      "tags": [
        {
          "tag": "Computer Science - Computer Vision and Pattern Recognition",
          "type": 1
        },
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Statistics - Machine Learning",
          "type": 1
        }
      ],
      "title": "Spectral Normalization for Generative Adversarial Networks",
      "url": "http://arxiv.org/abs/1802.05957"
    },
    {
      "abstractNote": "Recently, convolutional neural networks (CNNs) have been used as a powerful tool to solve many problems of machine learning and computer vision. In this paper, we aim to provide insight on the prop...",
      "accessDate": "2021-05-04T07:53:10Z",
      "conferenceName": "International Conference on Machine Learning",
      "creators": [
        {
          "creatorType": "author",
          "firstName": "Wenling",
          "lastName": "Shang"
        },
        {
          "creatorType": "author",
          "firstName": "Kihyuk",
          "lastName": "Sohn"
        },
        {
          "creatorType": "author",
          "firstName": "Diogo",
          "lastName": "Almeida"
        },
        {
          "creatorType": "author",
          "firstName": "Honglak",
          "lastName": "Lee"
        }
      ],
      "date": "2016/06/11",
      "extra": [
        "ISSN: 1938-7228"
      ],
      "itemID": 4,
      "itemType": "conferencePaper",
      "language": "en",
      "libraryCatalog": "proceedings.mlr.press",
      "pages": "2217-2225",
      "publicationTitle": "International Conference on Machine Learning",
      "publisher": "PMLR",
      "tags": [
        {
          "tag": "Residual Network"
        },
        {
          "tag": "Supervised Learning"
        }
      ],
      "title": "Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units",
      "url": "http://proceedings.mlr.press/v48/shang16.html"
    },
    {
      "abstractNote": "Recently, convolutional neural networks (CNNs) have been used as a powerful tool to solve many problems of machine learning and computer vision. In this paper, we aim to provide insight on the property of convolutional neural networks, as well as a generic method to improve the performance of many CNN architectures. Specifically, we first examine existing CNN models and observe an intriguing property that the filters in the lower layers form pairs (i.e., filters with opposite phase). Inspired by our observation, we propose a novel, simple yet effective activation scheme called concatenated ReLU (CRelu) and theoretically analyze its reconstruction property in CNNs. We integrate CRelu into several state-of-the-art CNN architectures and demonstrate improvement in their recognition performance on CIFAR-10/100 and ImageNet datasets with fewer trainable parameters. Our results suggest that better understanding of the properties of CNNs can lead to significant performance improvement with a simple modification.",
      "accessDate": "2019-09-27T17:02:27Z",
      "creators": [
        {
          "creatorType": "author",
          "firstName": "Wenling",
          "lastName": "Shang"
        },
        {
          "creatorType": "author",
          "firstName": "Kihyuk",
          "lastName": "Sohn"
        },
        {
          "creatorType": "author",
          "firstName": "Diogo",
          "lastName": "Almeida"
        },
        {
          "creatorType": "author",
          "firstName": "Honglak",
          "lastName": "Lee"
        }
      ],
      "date": "2016-03-16",
      "extra": [
        "arXiv: 1603.05201"
      ],
      "itemID": 5,
      "itemType": "journalArticle",
      "libraryCatalog": "arXiv.org",
      "notes": [
        "Comment: ICML 2016"
      ],
      "publicationTitle": "arXiv:1603.05201 [cs]",
      "tags": [
        {
          "tag": "Computer Science - Computer Vision and Pattern Recognition",
          "type": 1
        },
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        }
      ],
      "title": "Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units",
      "url": "http://arxiv.org/abs/1603.05201"
    },
    {
      "abstractNote": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.",
      "accessDate": "2020-08-20T10:09:22Z",
      "creators": [
        {
          "creatorType": "author",
          "firstName": "Kaiming",
          "lastName": "He"
        },
        {
          "creatorType": "author",
          "firstName": "Xiangyu",
          "lastName": "Zhang"
        },
        {
          "creatorType": "author",
          "firstName": "Shaoqing",
          "lastName": "Ren"
        },
        {
          "creatorType": "author",
          "firstName": "Jian",
          "lastName": "Sun"
        }
      ],
      "date": "2015-02-06",
      "extra": [
        "arXiv: 1502.01852"
      ],
      "itemID": 6,
      "itemType": "journalArticle",
      "libraryCatalog": "arXiv.org",
      "publicationTitle": "arXiv:1502.01852 [cs]",
      "shortTitle": "Delving Deep into Rectifiers",
      "tags": [
        {
          "tag": "Computer Science - Artificial Intelligence",
          "type": 1
        },
        {
          "tag": "Computer Science - Computer Vision and Pattern Recognition",
          "type": 1
        },
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        }
      ],
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
      "url": "http://arxiv.org/abs/1502.01852"
    },
    {
      "abstractNote": "SGD implicitly performs variational inference; gradient noise is highly non-isotropic, so SGD does not even converge to critical points of the original loss",
      "accessDate": "2022-02-06T15:41:40Z",
      "conferenceName": "International Conference on Learning Representations",
      "creators": [
        {
          "creatorType": "author",
          "firstName": "Pratik",
          "lastName": "Chaudhari"
        },
        {
          "creatorType": "author",
          "firstName": "Stefano",
          "lastName": "Soatto"
        }
      ],
      "date": "2018/02/15",
      "extra": [
        "00000"
      ],
      "itemID": 7,
      "itemType": "conferencePaper",
      "language": "en",
      "libraryCatalog": "openreview.net",
      "title": "Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",
      "url": "https://openreview.net/forum?id=HyWrIgW0W"
    },
    {
      "abstractNote": "Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been...",
      "accessDate": "2018-11-30T16:28:06Z",
      "creators": [
        {
          "creatorType": "author",
          "firstName": "Pratik",
          "lastName": "Chaudhari"
        },
        {
          "creatorType": "author",
          "firstName": "Stefano",
          "lastName": "Soatto"
        }
      ],
      "date": "2018/02/15",
      "itemID": 8,
      "itemType": "journalArticle",
      "libraryCatalog": "openreview.net",
      "title": "Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",
      "url": "https://openreview.net/forum?id=HyWrIgW0W"
    },
    {
      "abstractNote": "We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many...",
      "accessDate": "2022-05-25T10:30:26Z",
      "conferenceName": "International Conference on Learning Representations",
      "creators": [
        {
          "creatorType": "author",
          "firstName": "Hubert",
          "lastName": "Ramsauer"
        },
        {
          "creatorType": "author",
          "firstName": "Bernhard",
          "lastName": "Sch�fl"
        },
        {
          "creatorType": "author",
          "firstName": "Johannes",
          "lastName": "Lehner"
        },
        {
          "creatorType": "author",
          "firstName": "Philipp",
          "lastName": "Seidl"
        },
        {
          "creatorType": "author",
          "firstName": "Michael",
          "lastName": "Widrich"
        },
        {
          "creatorType": "author",
          "firstName": "Lukas",
          "lastName": "Gruber"
        },
        {
          "creatorType": "author",
          "firstName": "Markus",
          "lastName": "Holzleitner"
        },
        {
          "creatorType": "author",
          "firstName": "Thomas",
          "lastName": "Adler"
        },
        {
          "creatorType": "author",
          "firstName": "David",
          "lastName": "Kreil"
        },
        {
          "creatorType": "author",
          "firstName": "Michael K.",
          "lastName": "Kopp"
        },
        {
          "creatorType": "author",
          "firstName": "G�nter",
          "lastName": "Klambauer"
        },
        {
          "creatorType": "author",
          "firstName": "Johannes",
          "lastName": "Brandstetter"
        },
        {
          "creatorType": "author",
          "firstName": "Sepp",
          "lastName": "Hochreiter"
        }
      ],
      "date": "2020/09/28",
      "itemID": 9,
      "itemType": "conferencePaper",
      "language": "en",
      "libraryCatalog": "openreview.net",
      "title": "Hopfield Networks is All You Need",
      "url": "https://openreview.net/forum?id=tL89RnzIiCd"
    },
    {
      "abstractNote": "We propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator of GANs.",
      "accessDate": "2021-09-25T17:53:25Z",
      "conferenceName": "International Conference on Learning Representations",
      "creators": [
        {
          "creatorType": "author",
          "firstName": "Takeru",
          "lastName": "Miyato"
        },
        {
          "creatorType": "author",
          "firstName": "Toshiki",
          "lastName": "Kataoka"
        },
        {
          "creatorType": "author",
          "firstName": "Masanori",
          "lastName": "Koyama"
        },
        {
          "creatorType": "author",
          "firstName": "Yuichi",
          "lastName": "Yoshida"
        }
      ],
      "date": "2018/02/15",
      "itemID": 10,
      "itemType": "conferencePaper",
      "language": "en",
      "libraryCatalog": "openreview.net",
      "title": "Spectral Normalization for Generative Adversarial Networks",
      "url": "https://openreview.net/forum?id=B1QRgziT-"
    },
    {
      "abstractNote": "We show that the transformer attention mechanism is the update rule of a modern Hopfield network with continuous states. This new Hopfield network can store exponentially (with the dimension) many patterns, converges with one update, and has exponentially small retrieval errors. The number of stored patterns is traded off against convergence speed and retrieval error. The new Hopfield network has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. Transformer and BERT models operate in their first layers preferably in the global averaging regime, while they operate in higher layers in metastable states. The gradient in transformers is maximal for metastable states, is uniformly distributed for global averaging, and vanishes for a fixed point near a stored pattern. Using the Hopfield network interpretation, we analyzed learning of transformer and BERT models. Learning starts with attention heads that average and then most of them switch to metastable states. However, the majority of heads in the first layers still averages and can be replaced by averaging, e.g. our proposed Gaussian weighting. In contrast, heads in the last layers steadily learn and seem to use metastable states to collect information created in lower layers. These heads seem to be a promising target for improving transformers. Neural networks with Hopfield networks outperform other methods on immune repertoire classification, where the Hopfield net stores several hundreds of thousands of patterns. We provide a new PyTorch layer called \"Hopfield\", which allows to equip deep learning architectures with modern Hopfield networks as a new powerful concept comprising pooling, memory, and attention. GitHub: https://github.com/ml-jku/hopfield-layers",
      "accessDate": "2020-08-06T11:00:55Z",
      "creators": [
        {
          "creatorType": "author",
          "firstName": "Hubert",
          "lastName": "Ramsauer"
        },
        {
          "creatorType": "author",
          "firstName": "Bernhard",
          "lastName": "Sch�fl"
        },
        {
          "creatorType": "author",
          "firstName": "Johannes",
          "lastName": "Lehner"
        },
        {
          "creatorType": "author",
          "firstName": "Philipp",
          "lastName": "Seidl"
        },
        {
          "creatorType": "author",
          "firstName": "Michael",
          "lastName": "Widrich"
        },
        {
          "creatorType": "author",
          "firstName": "Lukas",
          "lastName": "Gruber"
        },
        {
          "creatorType": "author",
          "firstName": "Markus",
          "lastName": "Holzleitner"
        },
        {
          "creatorType": "author",
          "firstName": "Milena",
          "lastName": "Pavlovi"
        },
        {
          "creatorType": "author",
          "firstName": "Geir Kjetil",
          "lastName": "Sandve"
        },
        {
          "creatorType": "author",
          "firstName": "Victor",
          "lastName": "Greiff"
        },
        {
          "creatorType": "author",
          "firstName": "David",
          "lastName": "Kreil"
        },
        {
          "creatorType": "author",
          "firstName": "Michael",
          "lastName": "Kopp"
        },
        {
          "creatorType": "author",
          "firstName": "G�nter",
          "lastName": "Klambauer"
        },
        {
          "creatorType": "author",
          "firstName": "Johannes",
          "lastName": "Brandstetter"
        },
        {
          "creatorType": "author",
          "firstName": "Sepp",
          "lastName": "Hochreiter"
        }
      ],
      "date": "2020-07-16",
      "extra": [
        "arXiv: 2008.02217"
      ],
      "itemID": 11,
      "itemType": "journalArticle",
      "libraryCatalog": "arXiv.org",
      "notes": [
        "Comment: 10 pages (+ appendix); 9 figures; Companion paper with \"Modern Hopfield Networks and Attention for Immune Repertoire Classification\"; GitHub: https://github.com/ml-jku/hopfield-layers"
      ],
      "publicationTitle": "arXiv:2008.02217 [cs, stat]",
      "tags": [
        {
          "tag": "Computer Science - Computation and Language",
          "type": 1
        },
        {
          "tag": "Computer Science - Machine Learning",
          "type": 1
        },
        {
          "tag": "Computer Science - Neural and Evolutionary Computing",
          "type": 1
        },
        {
          "tag": "Statistics - Machine Learning",
          "type": 1
        }
      ],
      "title": "Hopfield Networks is All You Need",
      "url": "http://arxiv.org/abs/2008.02217"
    },
    {
      "accessDate": "2021-05-04T07:56:49Z",
      "conferenceName": "Proceedings of the IEEE International Conference on Computer Vision",
      "creators": [
        {
          "creatorType": "author",
          "firstName": "Kaiming",
          "lastName": "He"
        },
        {
          "creatorType": "author",
          "firstName": "Xiangyu",
          "lastName": "Zhang"
        },
        {
          "creatorType": "author",
          "firstName": "Shaoqing",
          "lastName": "Ren"
        },
        {
          "creatorType": "author",
          "firstName": "Jian",
          "lastName": "Sun"
        }
      ],
      "date": "2015",
      "itemID": 12,
      "itemType": "conferencePaper",
      "libraryCatalog": "openaccess.thecvf.com",
      "pages": "1026-1034",
      "shortTitle": "Delving Deep into Rectifiers",
      "tags": [
        {
          "tag": "Residual Network"
        },
        {
          "tag": "Supervised Learning"
        }
      ],
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
      "url": "https://openaccess.thecvf.com/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html"
    }
  ]
}