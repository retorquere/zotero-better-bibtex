@unpublished{_preprinthe_delving_deep_into_rectifiers_2015,
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human-Level Performance}} on {{ImageNet Classification}}},
  shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015-02-06},
  eprint = {1502.01852},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1502.01852},
  urldate = {2020-08-20},
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@unpublished{_preprintwang_on_solving_minimax_optimization_locally_2019,
  title = {On {{Solving Minimax Optimization Locally}}: {{A Follow-the-Ridge Approach}}},
  shorttitle = {On {{Solving Minimax Optimization Locally}}},
  author = {Wang, Yuanhao and Zhang, Guodong and Ba, Jimmy},
  date = {2019-11-25},
  eprint = {1910.07512},
  eprinttype = {arXiv},
  eprintclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1910.07512},
  urldate = {2019-12-18},
  abstract = {Many tasks in modern machine learning can be formulated as finding equilibria in \textbackslash emph\{sequential\} games. In particular, two-player zero-sum sequential games, also known as minimax optimization, have received growing interest. It is tempting to apply gradient descent to solve minimax optimization given its popularity and success in supervised learning. However, it has been noted that naive application of gradient descent fails to find some local minimax and can converge to non-local-minimax points. In this paper, we propose \textbackslash emph\{Follow-the-Ridge\} (FR), a novel algorithm that provably converges to and only converges to local minimax. We show theoretically that the algorithm addresses the notorious rotational behaviour of gradient dynamics, and is compatible with preconditioning and \textbackslash emph\{positive\} momentum. Empirically, FR solves toy minimax problems and improves the convergence of GAN training compared to the recent minimax optimization algorithms.},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  note = {Comment: 21 pages}
}

@inproceedings{chaudhari_stochastic_gradient_2018,
  title = {Stochastic Gradient Descent Performs Variational Inference, Converges to Limit Cycles for Deep Networks},
  author = {Chaudhari, Pratik and Soatto, Stefano},
  date = {2018-02-15},
  url = {https://openreview.net/forum?id=HyWrIgW0W},
  urldate = {2022-02-06},
  abstract = {SGD implicitly performs variational inference; gradient noise is highly non-isotropic, so SGD does not even converge to critical points of the original loss},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  annotation = {00000}
}

@article{chaudhari_stochastic_gradient_2018a,
  title = {Stochastic Gradient Descent Performs Variational Inference, Converges to Limit Cycles for Deep Networks},
  author = {Chaudhari, Pratik and Soatto, Stefano},
  date = {2018-02-15},
  url = {https://openreview.net/forum?id=HyWrIgW0W},
  urldate = {2018-11-30},
  abstract = {Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been...}
}

@inproceedings{he_delving_deep_2015,
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human-Level Performance}} on {{ImageNet Classification}}},
  shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015},
  pages = {1026--1034},
  url = {https://openaccess.thecvf.com/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html},
  urldate = {2021-05-04},
  eventtitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  keywords = {Residual Network,Supervised Learning}
}

@unpublished{miyato_spectral_normalization_2018,
  title = {Spectral {{Normalization}} for {{Generative Adversarial Networks}}},
  author = {Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
  date = {2018-02-16},
  eprint = {1802.05957},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1802.05957},
  urldate = {2021-09-25},
  abstract = {One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Published as a conference paper at ICLR 2018}
}

@inproceedings{miyato_spectral_normalization_2018a,
  title = {Spectral {{Normalization}} for {{Generative Adversarial Networks}}},
  author = {Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
  date = {2018-02-15},
  url = {https://openreview.net/forum?id=B1QRgziT-},
  urldate = {2021-09-25},
  abstract = {We propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator of GANs.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english}
}

@inproceedings{ramsauer_hopfield_networks_2020,
  title = {Hopfield {{Networks}} Is {{All You Need}}},
  author = {Ramsauer, Hubert and Sch�fl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Gruber, Lukas and Holzleitner, Markus and Adler, Thomas and Kreil, David and Kopp, Michael K. and Klambauer, G�nter and Brandstetter, Johannes and Hochreiter, Sepp},
  date = {2020-09-28},
  url = {https://openreview.net/forum?id=tL89RnzIiCd},
  urldate = {2022-05-25},
  abstract = {We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english}
}

@unpublished{ramsauer_hopfield_networks_2020a,
  title = {Hopfield {{Networks}} Is {{All You Need}}},
  author = {Ramsauer, Hubert and Sch�fl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Gruber, Lukas and Holzleitner, Markus and Pavlovi, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, G�nter and Brandstetter, Johannes and Hochreiter, Sepp},
  date = {2020-07-16},
  eprint = {2008.02217},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2008.02217},
  urldate = {2020-08-06},
  abstract = {We show that the transformer attention mechanism is the update rule of a modern Hopfield network with continuous states. This new Hopfield network can store exponentially (with the dimension) many patterns, converges with one update, and has exponentially small retrieval errors. The number of stored patterns is traded off against convergence speed and retrieval error. The new Hopfield network has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. Transformer and BERT models operate in their first layers preferably in the global averaging regime, while they operate in higher layers in metastable states. The gradient in transformers is maximal for metastable states, is uniformly distributed for global averaging, and vanishes for a fixed point near a stored pattern. Using the Hopfield network interpretation, we analyzed learning of transformer and BERT models. Learning starts with attention heads that average and then most of them switch to metastable states. However, the majority of heads in the first layers still averages and can be replaced by averaging, e.g. our proposed Gaussian weighting. In contrast, heads in the last layers steadily learn and seem to use metastable states to collect information created in lower layers. These heads seem to be a promising target for improving transformers. Neural networks with Hopfield networks outperform other methods on immune repertoire classification, where the Hopfield net stores several hundreds of thousands of patterns. We provide a new PyTorch layer called "Hopfield", which allows to equip deep learning architectures with modern Hopfield networks as a new powerful concept comprising pooling, memory, and attention. GitHub: https://github.com/ml-jku/hopfield-layers},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  note = {Comment: 10 pages (+ appendix); 9 figures; Companion paper with "Modern Hopfield Networks and Attention for Immune Repertoire Classification"; GitHub: https://github.com/ml-jku/hopfield-layers}
}

@inproceedings{shang_understanding_improving_2016,
  title = {Understanding and {{Improving Convolutional Neural Networks}} via {{Concatenated Rectified Linear Units}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Shang, Wenling and Sohn, Kihyuk and Almeida, Diogo and Lee, Honglak},
  date = {2016-06-11},
  pages = {2217--2225},
  publisher = {PMLR},
  issn = {1938-7228},
  url = {http://proceedings.mlr.press/v48/shang16.html},
  urldate = {2021-05-04},
  abstract = {Recently, convolutional neural networks (CNNs) have been used as a powerful tool to solve many problems of machine learning and computer vision. In this paper, we aim to provide insight on the prop...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {Residual Network,Supervised Learning}
}

@unpublished{shang_understanding_improving_2016a,
  title = {Understanding and {{Improving Convolutional Neural Networks}} via {{Concatenated Rectified Linear Units}}},
  author = {Shang, Wenling and Sohn, Kihyuk and Almeida, Diogo and Lee, Honglak},
  date = {2016-03-16},
  eprint = {1603.05201},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1603.05201},
  urldate = {2019-09-27},
  abstract = {Recently, convolutional neural networks (CNNs) have been used as a powerful tool to solve many problems of machine learning and computer vision. In this paper, we aim to provide insight on the property of convolutional neural networks, as well as a generic method to improve the performance of many CNN architectures. Specifically, we first examine existing CNN models and observe an intriguing property that the filters in the lower layers form pairs (i.e., filters with opposite phase). Inspired by our observation, we propose a novel, simple yet effective activation scheme called concatenated ReLU (CRelu) and theoretically analyze its reconstruction property in CNNs. We integrate CRelu into several state-of-the-art CNN architectures and demonstrate improvement in their recognition performance on CIFAR-10/100 and ImageNet datasets with fewer trainable parameters. Our results suggest that better understanding of the properties of CNNs can lead to significant performance improvement with a simple modification.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: ICML 2016}
}

@inproceedings{wang*_solving_minimax_2019,
  title = {On {{Solving Minimax Optimization Locally}}: {{A Follow-the-Ridge Approach}}},
  shorttitle = {On {{Solving Minimax Optimization Locally}}},
  author = {Wang*, Yuanhao and Zhang*, Guodong and Ba, Jimmy},
  date = {2019-09-25},
  url = {https://openreview.net/forum?id=Hkx7_1rKwS},
  urldate = {2022-02-08},
  abstract = {Many tasks in modern machine learning can be formulated as finding equilibria in sequential games. In particular, two-player zero-sum sequential games, also known as minimax optimization, have...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  annotation = {00060}
}
